---
title: "lab_11"
author: "derek willis"
date: "2023-04-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**note to self that this goes along with pre_lab_12**

## You will need

* Our usual libraries for working with data, plus rvest.

## Load libraries and establish settings

**Task** Create a codeblock and load appropriate packages and settings for this lab.

Let's get to scraping.

```{r}
library(tidyverse)
library(refinr)
library(janitor)
library(lubridate)
library(rvest)

```

## Questions

**Q1**. Scrape the listing of available Maryland state grants at https://grants.maryland.gov/Pages/StateGrants.aspx into a dataframe. 

```{r}
#save the URL as a variable

md_grants_url <- "https://grants.maryland.gov/Pages/StateGrants.aspx"

#inspect to determine html type
#type is table
```

You should have three columns, one of which is a date, so make sure the date column has a date datatype. 

```{r}

#create data frame from table

md_grants <- md_grants_url |> 
  read_html() |> 
  html_table()  

md_grants_clean <- bind_rows(md_grants) |> 
  clean_names() |> 
  mutate(due_date = mdy(due_date))

md_grants_clean

```


Then, write code to count the number of grants opportunities offered by each organization listed in your dataframe, showing the organization with the most grant opportunities first. Which state agency has the most?

**A1** 

Maryland Energy Administration

```{r}

md_grants_clean |> 
  group_by(organization) |> 
  summarise(count = n()) |> 
  arrange(desc(count))

```

**Q2** Next, let's scrape the list of press releases from Maryland's Office of the Public Defender, https://www.opd.state.md.us/press-releases. This isn't a table, so you'll need to use `html_elements()` and your browser's inspector and do some clean up on the results. 

#Disclaimer,  used chat GPT for help with how to handle duplicate rows, which lead me to the function distinct(). https://chat.openai.com/share/0249d320-5ea2-44c4-8592-06980cc140c3


```{r}
#save url
#save html as a variable
#class(pr_results)

pr_url <- "https://www.opd.state.md.us/press-releases"


pr_results <- pr_url |> 
  read_html()

pr_results_clean <- pr_results |> 
  html_elements('span.wixui-rich-text__text') |> 
  html_text() |> 
  as_tibble() |> 
  filter(value != "") |> 
  separate(value, c('date', 'title'), sep=":") |> 
  drop_na(title) |> 
  mutate(date = mdy(date)) |> 
  group_by(title) |> 
  distinct() #|> 
 # summarise(count = n()) |> 
 # arrange(desc(count))

pr_results_clean


```


The result should be a dataframe with two columns that contain the date and title, and the date column should have a date datatype. The challenge here is figuring out how to isolate the releases.

When you finish scraping into a dataframe, write code to find the press releases that have the word "police" in the title. How many are there and when was the most recent one?

**A2** 
There are nine press releases that have the work "police" in the title, and the most recent one, "in aftermath of video depicting excessive police force, public defender calls for the implementation of body worn cameras by ocean city police" was written on June 21, 2021.


```{r}

pr_police <- pr_results_clean |> 
  mutate(title = str_to_lower(title)) |> 
  filter(str_detect(title,"police")) |> 
  arrange(desc(date))

pr_police

```

**Q3** Sen. Ben Cardin, D-Maryland, has posted hundreds of press releases at https://www.cardin.senate.gov/?post_type=press-releases. It would be great to have all of them in a dataframe that has the following columns: date, title and url.

To do this, you will need to scrape the page's html and save that to a variable and _then_ extract the dates, titles and urls into _separate_ dataframes using html_elements(). 


#Disclaimer: ChatGPT & I became besties on this one
https://chat.openai.com/share/0249d320-5ea2-44c4-8592-06980cc140c3


```{r}
#save url
#save html as a variable

pr_url_bc <- "https://www.cardin.senate.gov/?post_type=press-releases"

#create value for date 
date <- pr_url_bc |> 
  read_html() |> 
  html_elements('.customBlog_item__date') |> 
  html_text()
date

#create value for titles and trim leading spaces 
titles <- pr_url_bc |> 
  read_html() |> 
  html_elements('.customBlog_item__title') |> 
  html_text() 

titles <- str_trim(titles)
titles
```


```{r}
#create value for urls

pr_urls_full <- read_html(pr_url_bc) |> 
  html_elements('.customBlog_item__readMore') |> 
  html_attr('href')

pr_urls_full

```

#create data frame combining the values

```{r}
press_release_full_df <- data.frame(date = date, title = titles, url = pr_urls_full) |> 
  mutate(date = mdy(date))

press_release_full_df
```

##Derek notes below##

And remember how we turn a list into a dataframe. The function `html_text()` pulls out the contents of a tag, but for urls we want the HTML attribute. Rvest gives you a way to extract the URL from a link; google to find out what it is.

At the end, you'll have three dataframes that you want to combine into a single dataframe. When we want to combine the rows of identical dataframes, we used `bind_rows()`. If you were combining columns instead of rows, there's a similar function. Find out what it is and use it to put all of the dataframes together into a single one.

When you're done, rename the columns so they make sense, then make sure the date column is an actual date.

Finally, tell me what questions you could ask of this data. Be creative.

**A3** 

I'd be curious to look at the frequency of press releases by year, month, and day of the week, and try and gauge whether there is a strategy. If there was a way to account for page views on the url posts, even better. If no-one is reading the releases, that is interesting, or if certain releases get significantly more views, then that is also intersting. 

I also look at trends by category. I'd start by running filters trying to categorize releases by key words and symbols, like: Cardin, $, Maryland, Energy, etc., until all releases are accounted for by a bucket. From there, I'd try and look at patterns or outliers that could be interesting stories. 

